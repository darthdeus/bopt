C:\dev\Miniconda3\python.exe C:/dev/master-thesis-code/optimize_cmdargs.py
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=0.48771412252434004', '--epsilon=0.2991275619150695', '--gamma=0.8368773597879496', '--episodes=300']
[2018-11-19 14:56:25,203] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -971.85
Episode 30, mean 100-episode return -960.4666666666667
Episode 40, mean 100-episode return -915.525
Episode 50, mean 100-episode return -892.42
Episode 60, mean 100-episode return -836.2166666666667
Episode 70, mean 100-episode return -790.4142857142857
Episode 80, mean 100-episode return -769.8875
Episode 90, mean 100-episode return -748.4222222222222
Episode 100, mean 100-episode return -722.86
Episode 110, mean 100-episode return -668.27
Episode 120, mean 100-episode return -652.12
Episode 130, mean 100-episode return -608.97
Episode 140, mean 100-episode return -590.35
Episode 150, mean 100-episode return -575.52
Episode 160, mean 100-episode return -576.18
Episode 170, mean 100-episode return -569.53
Episode 180, mean 100-episode return -564.91
Episode 190, mean 100-episode return -578.49
Episode 200, mean 100-episode return -573.27
Episode 210, mean 100-episode return -584.37
Episode 220, mean 100-episode return -559.93
Episode 230, mean 100-episode return -552.53
Episode 240, mean 100-episode return -539.99
Episode 250, mean 100-episode return -530.51
Episode 260, mean 100-episode return -508.18
Episode 270, mean 100-episode return -492.11
Episode 280, mean 100-episode return -463.2
Episode 290, mean 100-episode return -425.52
Episode 300, mean 100-episode return -428.05
Episode 310, mean 100-episode return -420.13
Episode 320, mean 100-episode return -402.29
Episode 330, mean 100-episode return -396.23
Episode 340, mean 100-episode return -385.21
Episode 350, mean 100-episode return -378.59
Episode 360, mean 100-episode return -376.98
Episode 370, mean 100-episode return -387.66
Episode 380, mean 100-episode return -397.87
Episode 390, mean 100-episode return -410.57
Episode 400, mean 100-episode return -403.59
Finished with: 0

***
b'The mean 100-episode return after evaluation -398.37\r\n'
*** Reward: -398.37
Found optimal params [ 1.         19.95895601]
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=2.0', '--epsilon=0.8', '--gamma=0.99', '--episodes=300']
[2018-11-19 14:56:33,081] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -1000.0
Episode 30, mean 100-episode return -1000.0
Episode 40, mean 100-episode return -1000.0
C:\dev\npfl122\labs\03\q_learning.py:64: RuntimeWarning: overflow encountered in double_scalars
  reward + args.gamma * np.max(Q[next_state, :]) - Q[state, action])
C:\dev\npfl122\labs\03\q_learning.py:64: RuntimeWarning: invalid value encountered in double_scalars
  reward + args.gamma * np.max(Q[next_state, :]) - Q[state, action])
C:\dev\Miniconda3\lib\site-packages\numpy\core\fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
Episode 50, mean 100-episode return -1000.0
Episode 60, mean 100-episode return -1000.0
Episode 70, mean 100-episode return -1000.0
Episode 80, mean 100-episode return -1000.0
Episode 90, mean 100-episode return -1000.0
Episode 100, mean 100-episode return -1000.0
Episode 110, mean 100-episode return -1000.0
Episode 120, mean 100-episode return -1000.0
Episode 130, mean 100-episode return -1000.0
Episode 140, mean 100-episode return -1000.0
Episode 150, mean 100-episode return -1000.0
Episode 160, mean 100-episode return -1000.0
Episode 170, mean 100-episode return -1000.0
Episode 180, mean 100-episode return -1000.0
Episode 190, mean 100-episode return -1000.0
Episode 200, mean 100-episode return -1000.0
Episode 210, mean 100-episode return -1000.0
Episode 220, mean 100-episode return -1000.0
Episode 230, mean 100-episode return -1000.0
Episode 240, mean 100-episode return -1000.0
Episode 250, mean 100-episode return -1000.0
Episode 260, mean 100-episode return -1000.0
Episode 270, mean 100-episode return -1000.0
Episode 280, mean 100-episode return -1000.0
Episode 290, mean 100-episode return -1000.0
Episode 300, mean 100-episode return -1000.0
Episode 310, mean 100-episode return -1000.0
Episode 320, mean 100-episode return -1000.0
Episode 330, mean 100-episode return -1000.0
Episode 340, mean 100-episode return -1000.0
Episode 350, mean 100-episode return -1000.0
Episode 360, mean 100-episode return -1000.0
Episode 370, mean 100-episode return -1000.0
Episode 380, mean 100-episode return -1000.0
Episode 390, mean 100-episode return -1000.0
Episode 400, mean 100-episode return -1000.0
Finished with: 0

***
b'The mean 100-episode return after evaluation -1000.0\r\n'
*** Reward: -1000.0
Found optimal params [ 1.84941374 10.15801576]
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=0.05', '--epsilon=0.03', '--gamma=0.8', '--episodes=300']
[2018-11-19 14:56:47,712] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -1000.0
Episode 30, mean 100-episode return -1000.0
Episode 40, mean 100-episode return -1000.0
Episode 50, mean 100-episode return -1000.0
Episode 60, mean 100-episode return -1000.0
Episode 70, mean 100-episode return -995.0
Episode 80, mean 100-episode return -995.625
Episode 90, mean 100-episode return -993.2666666666667
Episode 100, mean 100-episode return -990.98
Episode 110, mean 100-episode return -983.35
Episode 120, mean 100-episode return -976.42
Episode 130, mean 100-episode return -964.57
Episode 140, mean 100-episode return -949.02
Episode 150, mean 100-episode return -944.17
Episode 160, mean 100-episode return -923.61
Episode 170, mean 100-episode return -902.6
Episode 180, mean 100-episode return -869.63
Episode 190, mean 100-episode return -838.17
Episode 200, mean 100-episode return -804.95
Episode 210, mean 100-episode return -769.45
Episode 220, mean 100-episode return -740.86
Episode 230, mean 100-episode return -743.37
Episode 240, mean 100-episode return -720.44
Episode 250, mean 100-episode return -678.13
Episode 260, mean 100-episode return -643.86
Episode 270, mean 100-episode return -617.53
Episode 280, mean 100-episode return -602.28
Episode 290, mean 100-episode return -596.78
Episode 300, mean 100-episode return -586.67
Episode 310, mean 100-episode return -573.22
Episode 320, mean 100-episode return -557.4
Episode 330, mean 100-episode return -509.2
Episode 340, mean 100-episode return -496.29
Episode 350, mean 100-episode return -495.38
Episode 360, mean 100-episode return -500.17
Episode 370, mean 100-episode return -500.45
Episode 380, mean 100-episode return -491.28
Episode 390, mean 100-episode return -470.98
Episode 400, mean 100-episode return -471.08
Finished with: 0

***
b'The mean 100-episode return after evaluation -470.6\r\n'
*** Reward: -470.6
Found optimal params [1.2180452  6.82844475]
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=0.06317133087646296', '--epsilon=0.8', '--gamma=0.99', '--episodes=300']
[2018-11-19 14:56:58,013] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -1000.0
Episode 30, mean 100-episode return -1000.0
Episode 40, mean 100-episode return -1000.0
Episode 50, mean 100-episode return -1000.0
Episode 60, mean 100-episode return -998.5833333333334
Episode 70, mean 100-episode return -992.4142857142857
Episode 80, mean 100-episode return -983.4125
Episode 90, mean 100-episode return -979.9777777777778
Episode 100, mean 100-episode return -969.5
Episode 110, mean 100-episode return -963.09
Episode 120, mean 100-episode return -954.73
Episode 130, mean 100-episode return -941.06
Episode 140, mean 100-episode return -911.66
Episode 150, mean 100-episode return -879.96
Episode 160, mean 100-episode return -854.72
Episode 170, mean 100-episode return -827.91
Episode 180, mean 100-episode return -795.46
Episode 190, mean 100-episode return -775.09
Episode 200, mean 100-episode return -761.6
Episode 210, mean 100-episode return -726.74
Episode 220, mean 100-episode return -684.34
Episode 230, mean 100-episode return -650.84
Episode 240, mean 100-episode return -633.45
Episode 250, mean 100-episode return -618.79
Episode 260, mean 100-episode return -587.01
Episode 270, mean 100-episode return -564.4
Episode 280, mean 100-episode return -543.36
Episode 290, mean 100-episode return -524.6
Episode 300, mean 100-episode return -504.94
Episode 310, mean 100-episode return -482.66
Episode 320, mean 100-episode return -466.22
Episode 330, mean 100-episode return -444.7
Episode 340, mean 100-episode return -422.76
Episode 350, mean 100-episode return -401.75
Episode 360, mean 100-episode return -393.5
Episode 370, mean 100-episode return -378.46
Episode 380, mean 100-episode return -368.23
Episode 390, mean 100-episode return -343.37
Episode 400, mean 100-episode return -320.92
Finished with: 0

***
b'The mean 100-episode return after evaluation -320.59\r\n'
*** Reward: -320.59
Found optimal params [1.3223863  5.46382616]
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=0.4225109828169862', '--epsilon=0.7800675823204628', '--gamma=0.9893516208800347', '--episodes=300']
[2018-11-19 14:57:07,276] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -1000.0
Episode 30, mean 100-episode return -1000.0
Episode 40, mean 100-episode return -991.225
Episode 50, mean 100-episode return -971.88
Episode 60, mean 100-episode return -957.05
Episode 70, mean 100-episode return -947.3714285714286
Episode 80, mean 100-episode return -934.6625
Episode 90, mean 100-episode return -929.7
Episode 100, mean 100-episode return -921.51
Episode 110, mean 100-episode return -888.24
Episode 120, mean 100-episode return -874.46
Episode 130, mean 100-episode return -859.34
Episode 140, mean 100-episode return -837.36
Episode 150, mean 100-episode return -820.88
Episode 160, mean 100-episode return -797.47
Episode 170, mean 100-episode return -760.37
Episode 180, mean 100-episode return -721.26
Episode 190, mean 100-episode return -681.77
Episode 200, mean 100-episode return -661.13
Episode 210, mean 100-episode return -653.41
Episode 220, mean 100-episode return -598.47
Episode 230, mean 100-episode return -544.22
Episode 240, mean 100-episode return -502.87
Episode 250, mean 100-episode return -462.11
Episode 260, mean 100-episode return -430.91
Episode 270, mean 100-episode return -417.48
Episode 280, mean 100-episode return -401.26
Episode 290, mean 100-episode return -390.28
Episode 300, mean 100-episode return -352.28
Episode 310, mean 100-episode return -320.64
Episode 320, mean 100-episode return -317.98
Episode 330, mean 100-episode return -315.6
Episode 340, mean 100-episode return -308.73
Episode 350, mean 100-episode return -306.44
Episode 360, mean 100-episode return -298.88
Episode 370, mean 100-episode return -288.44
Episode 380, mean 100-episode return -285.75
Episode 390, mean 100-episode return -273.5
Episode 400, mean 100-episode return -273.44
Finished with: 0

***
b'The mean 100-episode return after evaluation -273.42\r\n'
*** Reward: -273.42
Found optimal params [1.33087978 6.02323401]
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=0.09722249178314027', '--epsilon=0.6454174181579992', '--gamma=0.9125630609923985', '--episodes=300']
[2018-11-19 14:57:15,526] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -1000.0
Episode 30, mean 100-episode return -1000.0
Episode 40, mean 100-episode return -1000.0
Episode 50, mean 100-episode return -999.62
Episode 60, mean 100-episode return -999.6833333333333
Episode 70, mean 100-episode return -982.3
Episode 80, mean 100-episode return -977.3
Episode 90, mean 100-episode return -954.6222222222223
Episode 100, mean 100-episode return -933.17
Episode 110, mean 100-episode return -906.06
Episode 120, mean 100-episode return -883.17
Episode 130, mean 100-episode return -855.83
Episode 140, mean 100-episode return -826.41
Episode 150, mean 100-episode return -795.88
Episode 160, mean 100-episode return -751.39
Episode 170, mean 100-episode return -727.47
Episode 180, mean 100-episode return -700.07
Episode 190, mean 100-episode return -696.56
Episode 200, mean 100-episode return -672.43
Episode 210, mean 100-episode return -645.27
Episode 220, mean 100-episode return -612.35
Episode 230, mean 100-episode return -584.29
Episode 240, mean 100-episode return -557.01
Episode 250, mean 100-episode return -528.25
Episode 260, mean 100-episode return -511.47
Episode 270, mean 100-episode return -479.88
Episode 280, mean 100-episode return -449.47
Episode 290, mean 100-episode return -411.81
Episode 300, mean 100-episode return -393.8
Episode 310, mean 100-episode return -383.63
Episode 320, mean 100-episode return -378.05
Episode 330, mean 100-episode return -372.69
Episode 340, mean 100-episode return -375.74
Episode 350, mean 100-episode return -372.81
Episode 360, mean 100-episode return -372.51
Episode 370, mean 100-episode return -374.23
Episode 380, mean 100-episode return -376.53
Episode 390, mean 100-episode return -374.82
Episode 400, mean 100-episode return -381.49
Finished with: 0

***
b'The mean 100-episode return after evaluation -382.4\r\n'
*** Reward: -382.4
Found optimal params [1.28723876 6.93235887]
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=1.2336607220545515', '--epsilon=0.29694834406281034', '--gamma=0.9088591020640273', '--episodes=300']
[2018-11-19 14:57:24,243] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -1000.0
Episode 30, mean 100-episode return -1000.0
Episode 40, mean 100-episode return -1000.0
Episode 50, mean 100-episode return -1000.0
Episode 60, mean 100-episode return -1000.0
Episode 70, mean 100-episode return -1000.0
Episode 80, mean 100-episode return -1000.0
Episode 90, mean 100-episode return -1000.0
Episode 100, mean 100-episode return -1000.0
Episode 110, mean 100-episode return -1000.0
Episode 120, mean 100-episode return -1000.0
Episode 130, mean 100-episode return -1000.0
Episode 140, mean 100-episode return -1000.0
Episode 150, mean 100-episode return -1000.0
Episode 160, mean 100-episode return -1000.0
Episode 170, mean 100-episode return -1000.0
Episode 180, mean 100-episode return -1000.0
Episode 190, mean 100-episode return -1000.0
Episode 200, mean 100-episode return -1000.0
Episode 210, mean 100-episode return -1000.0
Episode 220, mean 100-episode return -1000.0
Episode 230, mean 100-episode return -1000.0
Episode 240, mean 100-episode return -1000.0
Episode 250, mean 100-episode return -1000.0
Episode 260, mean 100-episode return -1000.0
Episode 270, mean 100-episode return -1000.0
Episode 280, mean 100-episode return -1000.0
Episode 290, mean 100-episode return -1000.0
Episode 300, mean 100-episode return -1000.0
Episode 310, mean 100-episode return -1000.0
Episode 320, mean 100-episode return -1000.0
Episode 330, mean 100-episode return -1000.0
Episode 340, mean 100-episode return -1000.0
Episode 350, mean 100-episode return -1000.0
Episode 360, mean 100-episode return -1000.0
Episode 370, mean 100-episode return -1000.0
Episode 380, mean 100-episode return -1000.0
Episode 390, mean 100-episode return -1000.0
Episode 400, mean 100-episode return -1000.0
Finished with: 0

***
b'The mean 100-episode return after evaluation -1000.0\r\n'
*** Reward: -1000.0
Found optimal params [0.89013597 4.66721393]
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=0.20654171515497965', '--epsilon=0.1584273192213486', '--gamma=0.899745502415843', '--episodes=300']
[2018-11-19 14:57:37,944] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -1000.0
Episode 30, mean 100-episode return -1000.0
Episode 40, mean 100-episode return -951.325
Episode 50, mean 100-episode return -906.78
Episode 60, mean 100-episode return -909.85
Episode 70, mean 100-episode return -889.1428571428571
Episode 80, mean 100-episode return -863.45
Episode 90, mean 100-episode return -816.9666666666667
Episode 100, mean 100-episode return -788.89
Episode 110, mean 100-episode return -741.67
Episode 120, mean 100-episode return -691.92
Episode 130, mean 100-episode return -632.62
Episode 140, mean 100-episode return -601.01
Episode 150, mean 100-episode return -587.1
Episode 160, mean 100-episode return -540.82
Episode 170, mean 100-episode return -505.83
Episode 180, mean 100-episode return -472.01
Episode 190, mean 100-episode return -466.24
Episode 200, mean 100-episode return -467.21
Episode 210, mean 100-episode return -478.37
Episode 220, mean 100-episode return -479.51
Episode 230, mean 100-episode return -472.87
Episode 240, mean 100-episode return -461.2
Episode 250, mean 100-episode return -459.92
Episode 260, mean 100-episode return -460.12
Episode 270, mean 100-episode return -458.03
Episode 280, mean 100-episode return -461.67
Episode 290, mean 100-episode return -457.17
Episode 300, mean 100-episode return -437.02
Episode 310, mean 100-episode return -454.62
Episode 320, mean 100-episode return -488.85
Episode 330, mean 100-episode return -546.7
Episode 340, mean 100-episode return -592.59
Episode 350, mean 100-episode return -613.14
Episode 360, mean 100-episode return -641.01
Episode 370, mean 100-episode return -680.99
Episode 380, mean 100-episode return -719.44
Episode 390, mean 100-episode return -776.89
Episode 400, mean 100-episode return -825.62
Finished with: 0

***
b'The mean 100-episode return after evaluation -832.38\r\n'
*** Reward: -832.38
Found optimal params [1.00000000e-05 2.05187196e+00]
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=1.3103007096793646', '--epsilon=0.6160335879333009', '--gamma=0.9546659225198009', '--episodes=300']
[2018-11-19 14:57:46,510] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -1000.0
Episode 30, mean 100-episode return -1000.0
Episode 40, mean 100-episode return -1000.0
Episode 50, mean 100-episode return -1000.0
Episode 60, mean 100-episode return -1000.0
Episode 70, mean 100-episode return -1000.0
Episode 80, mean 100-episode return -1000.0
Episode 90, mean 100-episode return -1000.0
Episode 100, mean 100-episode return -1000.0
Episode 110, mean 100-episode return -1000.0
Episode 120, mean 100-episode return -1000.0
Episode 130, mean 100-episode return -1000.0
Episode 140, mean 100-episode return -1000.0
Episode 150, mean 100-episode return -1000.0
Episode 160, mean 100-episode return -1000.0
Episode 170, mean 100-episode return -1000.0
Episode 180, mean 100-episode return -1000.0
Episode 190, mean 100-episode return -1000.0
Episode 200, mean 100-episode return -1000.0
Episode 210, mean 100-episode return -1000.0
Episode 220, mean 100-episode return -1000.0
Episode 230, mean 100-episode return -1000.0
Episode 240, mean 100-episode return -1000.0
Episode 250, mean 100-episode return -1000.0
Episode 260, mean 100-episode return -1000.0
Episode 270, mean 100-episode return -1000.0
Episode 280, mean 100-episode return -1000.0
Episode 290, mean 100-episode return -1000.0
Episode 300, mean 100-episode return -1000.0
Episode 310, mean 100-episode return -1000.0
Episode 320, mean 100-episode return -1000.0
Episode 330, mean 100-episode return -1000.0
Episode 340, mean 100-episode return -1000.0
Episode 350, mean 100-episode return -1000.0
Episode 360, mean 100-episode return -1000.0
Episode 370, mean 100-episode return -1000.0
Episode 380, mean 100-episode return -1000.0
Episode 390, mean 100-episode return -1000.0
Episode 400, mean 100-episode return -1000.0
Finished with: 0

***
b'The mean 100-episode return after evaluation -1000.0\r\n'
*** Reward: -1000.0
Found optimal params [1.00000000e-05 1.92246235e+00]
Running: ['python', 'C:\\dev\\npfl122\\labs\\03\\q_learning.py', '--alpha=0.79703917250756', '--epsilon=0.22075490634208858', '--gamma=0.8818935686295608', '--episodes=300']
[2018-11-19 14:57:59,964] Making new env: MountainCarLimit1000-v0
Episode 10, mean 100-episode return -1000.0
Episode 20, mean 100-episode return -969.45
Episode 30, mean 100-episode return -925.8666666666667
Episode 40, mean 100-episode return -905.125
Episode 50, mean 100-episode return -892.96
Episode 60, mean 100-episode return -903.4333333333333
Episode 70, mean 100-episode return -872.3285714285714
Episode 80, mean 100-episode return -857.5625
Episode 90, mean 100-episode return -845.0555555555555
Episode 100, mean 100-episode return -838.66
Episode 110, mean 100-episode return -809.36
Episode 120, mean 100-episode return -780.05
Episode 130, mean 100-episode return -776.55
Episode 140, mean 100-episode return -752.06
Episode 150, mean 100-episode return -736.55
Episode 160, mean 100-episode return -716.92
Episode 170, mean 100-episode return -710.23
Episode 180, mean 100-episode return -699.09
Episode 190, mean 100-episode return -717.84
Episode 200, mean 100-episode return -726.06
Episode 210, mean 100-episode return -750.15
Episode 220, mean 100-episode return -748.57
Episode 230, mean 100-episode return -737.99
Episode 240, mean 100-episode return -725.14
Episode 250, mean 100-episode return -705.32
Episode 260, mean 100-episode return -672.5
Episode 270, mean 100-episode return -644.55
Episode 280, mean 100-episode return -610.04
Episode 290, mean 100-episode return -555.21
Episode 300, mean 100-episode return -533.64
Episode 310, mean 100-episode return -466.98
Episode 320, mean 100-episode return -438.5
Episode 330, mean 100-episode return -399.89
Episode 340, mean 100-episode return -384.55
Episode 350, mean 100-episode return -366.15
Episode 360, mean 100-episode return -354.77
Episode 370, mean 100-episode return -352.76
Episode 380, mean 100-episode return -352.11
Episode 390, mean 100-episode return -351.57
Episode 400, mean 100-episode return -314.03
Finished with: 0

***
b'The mean 100-episode return after evaluation -313.17\r\n'
*** Reward: -313.17
max_x [0.42251098 0.78006758 0.98935162] max max -273.42




****************************
OptimizationResult(best_x=[0.42251098 0.78006758 0.98935162], best_y=-273.42)

Process finished with exit code 0
